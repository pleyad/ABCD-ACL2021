"""Runs Evaluation for ABCD reproduction attempt.

This module was created by the ABCDre team, as there were no evaluation scripts in the
Repository of the original paper. Most metrics were underdefined, so we implement them
based on research standards and intuitions.
"""

import argparse
import os
import re
from sacrebleu.metrics import BLEU
from bert_score import BERTScorer
from typing import Tuple, List

import logging


def file_path(path):
    if os.path.isfile(path):
        return path
    else:
        raise FileNotFoundError(path)


# Argument parser
parser = argparse.ArgumentParser()
parser.add_argument(
    "--reference",
    help="Path to the file that contains the gold sentences (one per line)",
    type=file_path,
    required=True,
)
parser.add_argument(
    "--hypothesis",
    help="Path to the file that contains the sentences generated by the model (one "
    "per line)",
    type=file_path,
    required=True,
)
parser.add_argument(
    "--modelname",
    help="Name of the model that is being evaluated",
    type=str,
    required=True,
)
parser.add_argument(
    "--dataset",
    help="Name of the dataset on which the model is being evaluated",
    type=str,
    required=True,
)

args = parser.parse_args()


def extract_sentences(
    ref_path: os.PathLike, hyp_path: os.PathLike
) -> Tuple[List[List[str]], List[List[str]]]:
    with open(ref_path, "r", encoding="utf-8") as r, open(
        hyp_path, "r", encoding="utf-8"
    ) as h:
        ref_lines = r.readlines()
        hyp_lines = h.readlines()

        assert len(ref_lines) == len(hyp_lines)  # sanity check

        split_ref_lines = [re.split(r"(?<=\.) ", line.strip()) for line in ref_lines]
        split_hyp_lines = [re.split(r"(?<=\.) ", line.strip()) for line in hyp_lines]

    return split_ref_lines, split_hyp_lines


def compute_average_num_tok_per_ss(split_lines: List[List[str]]) -> float:
    """Computes the average number of tokens per simple sentence"""
    sent_count = 0
    total_tokens = 0

    for line in split_lines:
        for sent in line:
            sent_count += 1
            # Assuming final period counts as token too
            total_tokens += len(sent.split())

    return total_tokens / sent_count


def compute_match_num_ss(
    split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the percentage of sentences in which the predicted number of simple sentences is correct"""
    correct_num_of_sents = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        if len(hyp_line) == len(ref_line):
            correct_num_of_sents += 1

    return correct_num_of_sents / len(split_ref_lines)


def compute_bleu_score(
    split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the average BLEU score"""
    bleu = BLEU(
        effective_order=True
    )  # This is recommended by sacreBLEU, no idea if it was activated in original paper
    total_bleus = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        num_sents = max(len(ref_line), len(hyp_line))
        total_bleu = 0
        # aligning based on indices; we might need aligner
        for hyp, ref in zip(hyp_line, ref_line):
            # up to 4grams is default; we might want ONLY 4grams
            bleu_score = bleu.sentence_score(hyp, [ref]).score
            total_bleu += bleu_score

        line_average = total_bleu / num_sents
        total_bleus += line_average

    return total_bleus / len(split_ref_lines)


def compute_bert_score(
    split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the average BERT score"""
    scorer = BERTScorer(lang="en")  # config for scorer not clear
    total_scores = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        num_sents = max(len(ref_line), len(hyp_line))
        total_score = 0
        # aligning based on indices; we might need aligner
        for hyp, ref in zip(hyp_line, ref_line):
            prec, rec, f1 = scorer.score([hyp], [ref])
            # not clear which metric is being reported in paper, might be precision or recall
            total_score += f1.item()

        line_average = total_score / num_sents
        total_scores += line_average

    return total_scores / len(split_ref_lines)


def main():

    print(args)
    
    ref_lines, hyp_lines = extract_sentences(args.reference, args.hypothesis)

    match_num_ss = compute_match_num_ss(ref_lines, hyp_lines)
    average_num_tok_per_ss_hyp = compute_average_num_tok_per_ss(hyp_lines)
    average_num_tok_per_ss_ref = compute_average_num_tok_per_ss(ref_lines)
    bleu_score = compute_bleu_score(ref_lines, hyp_lines)
    bert_score = compute_bert_score(ref_lines, hyp_lines)

    print("-----------------------------\n")
    print(
        f"The scores for the model {args.modelname} on test set {args.dataset} are:\n"
    )
    print(
        f"#T/SS: {average_num_tok_per_ss_hyp} (in reference: {average_num_tok_per_ss_ref})\n"
    )
    print(f"Match #SS%: {match_num_ss}\n")
    print(f"BLEU score: {bleu_score}\n")
    print(f"BERT score: {bert_score}\n")


if __name__ == "__main__":
    main()
