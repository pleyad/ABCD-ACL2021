"""Runs Evaluation for ABCD reproduction attempt.

This module was created by the ABCDre team, as there were no evaluation scripts in the
Repository of the original paper. Most metrics were underdefined, so we implement them
based on research standards and intuition.
"""

import argparse
import os
import re
import itertools
from sacrebleu.metrics import BLEU
from bert_score import BERTScorer
from typing import Tuple, List
from sentence_transformers import SentenceTransformer, util

import logging


def file_path(path):
    if os.path.isfile(path):
        return path
    else:
        raise FileNotFoundError(path)


# Argument parser
parser = argparse.ArgumentParser()
parser.add_argument(
    "--reference",
    help="Path to the file that contains the gold sentences (one per line)",
    type=file_path,
    required=True,
)
parser.add_argument(
    "--hypothesis",
    help="Path to the file that contains the sentences generated by the model (one "
         "per line)",
    type=file_path,
    required=True,
)
parser.add_argument(
    "--modelname",
    help="Name of the model that is being evaluated",
    type=str,
    required=True,
)
parser.add_argument(
    "--dataset",
    help="Name of the dataset on which the model is being evaluated",
    type=str,
    required=True,
)

args = parser.parse_args()


def extract_sentences(
        ref_path: os.PathLike, hyp_path: os.PathLike
) -> Tuple[List[List[str]], List[List[str]]]:
    with open(ref_path, "r", encoding="utf-8") as r, open(
            hyp_path, "r", encoding="utf-8"
    ) as h:
        ref_lines = r.readlines()
        hyp_lines = h.readlines()

        assert len(ref_lines) == len(hyp_lines)  # sanity check

        split_ref_lines = [re.split(r"(?<=\.) ", line.strip()) for line in ref_lines]
        split_hyp_lines = [re.split(r"(?<=\.) ", line.strip()) for line in hyp_lines]

    return split_ref_lines, split_hyp_lines


def compute_average_num_tok_per_ss(split_lines: List[List[str]]) -> float:
    """Computes the average number of tokens per simple sentence"""
    sent_count = 0
    total_tokens = 0

    for line in split_lines:
        for sent in line:
            sent_count += 1
            # Assuming final period counts as token too
            total_tokens += len(sent.split())

    return total_tokens / sent_count


def compute_match_num_ss(
        split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the percentage of sentences in which the predicted number of simple sentences is correct"""
    correct_num_of_sents = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        if len(hyp_line) == len(ref_line):
            correct_num_of_sents += 1

    return correct_num_of_sents / len(split_ref_lines)


def align_sentences(split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]) -> Tuple[List[List[str]], List[List[str]]]:
    """
    Aligns simple sentences in ref and hyp based on cosine-similarity of sentence embedding.
    In case of non-matching number of sentences, unmatched sentences are placed at the end
    so that we can assume alignment based on index in later computations and chop off unmatched
    sentences at the end.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')

    aligned_ref_lines = []
    aligned_hyp_lines = []

    debug = False

    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        # Compute embedding for both lists
        embeddings_ref = model.encode(ref_line, convert_to_tensor=True)
        embeddings_hyp = model.encode(hyp_line, convert_to_tensor=True)

        # Compute cosine-similarities for each sentence with each other sentence
        # returns matrix of shape len(ref_line) x len(hyp_line)
        cosine_scores = util.cos_sim(embeddings_ref, embeddings_hyp)

        # Align best-matching sentences:
        # Loop over all possible alignments and compute score, enforcing that each sentence must be aligned with
        # another sentence unless either reference or hypothesis has more simple sentences than the other one
        # in which case "leftover" sentences are not aligned

        max_score = float("-inf")
        ref_permutation = None
        hyp_permutation = None

        if debug and len(ref_line) != len(hyp_line) and min(len(ref_line), len(hyp_line)) >= 2:
            print(cosine_scores)

        # Case 1: number of sentences in ref is smaller of equal to number of sentences in hyp
        if len(ref_line) <= len(hyp_line):
            for permutation in itertools.permutations(range(len(hyp_line))):
                total_score = 0
                for i in range(len(ref_line)):
                    total_score += cosine_scores[i, permutation[i]].item()
                if total_score > max_score:
                    max_score = total_score
                    ref_permutation = range(len(ref_line))
                    hyp_permutation = permutation

        # Case 2: number of sentences in hyp is smaller than number of sentences in ref
        else:
            for permutation in itertools.permutations(range(len(ref_line))):
                total_score = 0
                for i in range(len(hyp_line)):
                    total_score += cosine_scores[permutation[i], i].item()
                if total_score > max_score:
                    max_score = total_score
                    ref_permutation = permutation
                    hyp_permutation = range(len(hyp_line))

        if debug and len(ref_line) != len(hyp_line)  and min(len(ref_line), len(hyp_line)) >= 2:
            print(list(ref_permutation))
            print(list(hyp_permutation))

        # Rearrange simple sentences to match with the highest score alignment
        aligned_ref_line = [ref_line[i] for i in ref_permutation]
        aligned_hyp_line = [hyp_line[i] for i in hyp_permutation]
        aligned_ref_lines.append(aligned_ref_line)
        aligned_hyp_lines.append(aligned_hyp_line)

    return aligned_ref_lines, aligned_hyp_lines


def compute_bleu_score(
        split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the average BLEU score"""
    bleu = BLEU(
        effective_order=True
    )  # This is recommended by sacreBLEU, no idea if it was activated in original paper
    total_bleus = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        num_sents = max(len(ref_line), len(hyp_line))
        total_bleu = 0
        # aligning based on indices; we might need aligner
        for hyp, ref in zip(hyp_line, ref_line):
            # up to 4grams is default; we might want ONLY 4grams ?
            bleu_score = bleu.sentence_score(hyp, [ref]).score
            total_bleu += bleu_score

        line_average = total_bleu / num_sents
        total_bleus += line_average

    return total_bleus / len(split_ref_lines)


def compute_bert_score(
        split_ref_lines: List[List[str]], split_hyp_lines: List[List[str]]
) -> float:
    """Computes the average BERT score"""
    scorer = BERTScorer(lang="en")  # config for scorer not clear
    total_scores = 0
    for ref_line, hyp_line in zip(split_ref_lines, split_hyp_lines):
        num_sents = max(len(ref_line), len(hyp_line))
        total_score = 0
        # aligning based on indices; we might need aligner
        for hyp, ref in zip(hyp_line, ref_line):
            prec, rec, f1 = scorer.score([hyp], [ref])
            # not clear which metric is being reported in paper, might be precision or recall
            total_score += f1.item()

        line_average = total_score / num_sents
        total_scores += line_average

    return total_scores / len(split_ref_lines)


def main():
    ref_lines, hyp_lines = extract_sentences(args.reference, args.hypothesis)
    ref_lines, hyp_lines = align_sentences(ref_lines, hyp_lines)

    match_num_ss = compute_match_num_ss(ref_lines, hyp_lines)
    average_num_tok_per_ss_hyp = compute_average_num_tok_per_ss(hyp_lines)
    average_num_tok_per_ss_ref = compute_average_num_tok_per_ss(ref_lines)
    bleu_score = compute_bleu_score(ref_lines, hyp_lines)
    bert_score = compute_bert_score(ref_lines, hyp_lines)

    print("-----------------------------\n")
    print(
        f"The scores for the model {args.modelname} on test set {args.dataset} are:\n"
    )
    print(
        f"#T/SS: {average_num_tok_per_ss_hyp} (in reference: {average_num_tok_per_ss_ref})\n"
    )
    print(f"Match #SS%: {match_num_ss}\n")
    print(f"BLEU score: {bleu_score}\n")
    print(f"BERT score: {bert_score}\n")


if __name__ == "__main__":
    main()
